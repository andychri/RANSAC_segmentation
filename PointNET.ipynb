{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594eba86-eb72-4ab3-8539-83ed620408d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 09:39:21.248030: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-02 09:39:21.280519: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 09:39:21.280543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 09:39:21.281468: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 09:39:21.285851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 09:39:21.947486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define PointNet architecture\n",
    "def PointNet(num_classes):\n",
    "    input_points = layers.Input(shape=(None, 3))  # Input: point cloud of shape (num_points, 3)\n",
    "\n",
    "    # MLP to process point cloud\n",
    "    x = layers.Conv1D(64, 1, activation='relu')(input_points)\n",
    "    x = layers.Conv1D(128, 1, activation='relu')(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu')(x)\n",
    "\n",
    "    # Max pooling to get global feature\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)  # Output: class scores\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = models.Model(inputs=input_points, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "model = PointNet(num_classes=40)  # ModelNet40 has 40 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634761d0-c016-4a21-87e1-9224442ee9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x712e2c7a7640>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andreas/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x712e2c7a7640>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andreas/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x712e2c7a7640>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andreas/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x712e2c7a7640>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andreas/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess a single .off file as a point cloud\n",
    "def load_off_file(off_filename, num_points=1024):\n",
    "    # Load the mesh using trimesh\n",
    "    mesh = trimesh.load_mesh(off_filename)\n",
    "    \n",
    "    # Sample points from the mesh\n",
    "    point_cloud = mesh.sample(num_points)\n",
    "    \n",
    "    # Normalize the point cloud (optional, but typically done)\n",
    "    point_cloud -= np.mean(point_cloud, axis=0)  # Centering\n",
    "    point_cloud /= np.max(np.linalg.norm(point_cloud, axis=1))  # Scaling\n",
    "    \n",
    "    return point_cloud\n",
    "\n",
    "# Function to load all .off files in a directory and their labels\n",
    "def load_off_dataset(directory, num_points=1024):\n",
    "    point_clouds = []\n",
    "    labels = []\n",
    "    \n",
    "    # Walk through the directory to find all .off files\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.off'):\n",
    "                # Full path to the file\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                # Extract label from the subdirectory (e.g., 'bottle' or 'chair')\n",
    "                label = os.path.basename(subdir)\n",
    "                \n",
    "                # Load the point cloud from the .off file\n",
    "                point_cloud = load_off_file(file_path, num_points=num_points)\n",
    "                \n",
    "                # Append the point cloud and label\n",
    "                point_clouds.append(point_cloud)\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    point_clouds = np.array(point_clouds)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return point_clouds, labels\n",
    "\n",
    "# Example of loading the dataset\n",
    "dataset_dir = 'archive/ModelNet40/'\n",
    "train_data, train_labels = load_off_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b27e8f-a969-4343-9261-726fc49a4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (12311, 1024, 3)\n",
      "Train labels shape: (12311,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess: Randomly shuffle the data\n",
    "indices = np.arange(len(train_labels))\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_labels = train_labels[indices]\n",
    "\n",
    "# Check the shapes of the data\n",
    "print(\"Train data shape:\", train_data.shape)  # Should be (num_samples, 1024, 3)\n",
    "print(\"Train labels shape:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "449a2a5a-c92a-4d33-8904-30acd157ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    'airplane': 0,\n",
    "    'bathtub': 1,\n",
    "    'bed': 2,\n",
    "    'bench': 3,\n",
    "    'bookshelf': 4,\n",
    "    'bottle': 5,\n",
    "    'bowl': 6,\n",
    "    'car': 7,\n",
    "    'chair': 8,\n",
    "    'cone': 9,\n",
    "    'cup': 10,\n",
    "    'curtain': 11,\n",
    "    'desk': 12,\n",
    "    'door': 13,\n",
    "    'dresser': 14,\n",
    "    'flower_pot': 15,\n",
    "    'glass_box': 16,\n",
    "    'guitar': 17,\n",
    "    'keyboard': 18,\n",
    "    'lamp': 19,\n",
    "    'laptop': 20,\n",
    "    'mantel': 21,\n",
    "    'monitor': 22,\n",
    "    'night_stand': 23,\n",
    "    'person': 24,\n",
    "    'piano': 25,\n",
    "    'plant': 26,\n",
    "    'radio': 27,\n",
    "    'range_hood': 28,\n",
    "    'sink': 29,\n",
    "    'sofa': 30,\n",
    "    'stairs': 31,\n",
    "    'stool': 32,\n",
    "    'table': 33,\n",
    "    'tent': 34,\n",
    "    'toilet': 35,\n",
    "    'tv_stand': 36,\n",
    "    'vase': 37,\n",
    "    'wardrobe': 38,\n",
    "    'xbox': 39\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5598b584-b77a-40fe-9ddd-07dab46d018d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Example of loading the dataset\u001b[39;00m\n\u001b[1;32m     96\u001b[0m dataset_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive/ModelNet40/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 97\u001b[0m train_data, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_off_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Shuffle the data\u001b[39;00m\n\u001b[1;32m    100\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(train_labels))\n",
      "Cell \u001b[0;32mIn[13], line 80\u001b[0m, in \u001b[0;36mload_off_dataset\u001b[0;34m(directory, num_points)\u001b[0m\n\u001b[1;32m     77\u001b[0m label_str \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(subdir)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Convert label string to a numerical label using the mapping\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mLABEL_MAP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_str\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Load the point cloud from the .off file\u001b[39;00m\n\u001b[1;32m     83\u001b[0m point_cloud \u001b[38;5;241m=\u001b[39m load_off_file(file_path, num_points\u001b[38;5;241m=\u001b[39mnum_points)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess a single .off file as a point cloud\n",
    "def load_off_file(off_filename, num_points=1024):\n",
    "    # Load the mesh using trimesh\n",
    "    mesh = trimesh.load_mesh(off_filename)\n",
    "    \n",
    "    # Sample points from the mesh\n",
    "    point_cloud = mesh.sample(num_points)\n",
    "    \n",
    "    # Normalize the point cloud (optional, but typically done)\n",
    "    point_cloud -= np.mean(point_cloud, axis=0)  # Centering\n",
    "    point_cloud /= np.max(np.linalg.norm(point_cloud, axis=1))  # Scaling\n",
    "    \n",
    "    return point_cloud\n",
    "\n",
    "# Function to load all .off files in a directory and their labels\n",
    "def load_off_dataset(directory, num_points=1024):\n",
    "    point_clouds = []\n",
    "    labels = []\n",
    "    \n",
    "    # Walk through the directory to find all .off files\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.off'):\n",
    "                # Full path to the file\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                # Extract label from the subdirectory (e.g., 'bottle' or 'chair')\n",
    "                label_str = os.path.basename(subdir)\n",
    "                \n",
    "                # Convert label string to a numerical label using the mapping\n",
    "                label = LABEL_MAP[label_str]\n",
    "                \n",
    "                # Load the point cloud from the .off file\n",
    "                point_cloud = load_off_file(file_path, num_points=num_points)\n",
    "                \n",
    "                # Append the point cloud and label\n",
    "                point_clouds.append(point_cloud)\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    point_clouds = np.array(point_clouds)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return point_clouds, labels\n",
    "\n",
    "# Example of loading the dataset\n",
    "dataset_dir = 'archive/ModelNet40/'\n",
    "train_data, train_labels = load_off_dataset(dataset_dir)\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(len(train_labels))\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_labels = train_labels[indices]\n",
    "\n",
    "# Check the shapes of the data\n",
    "print(\"Train data shape:\", train_data.shape)  # Should be (num_samples, 1024, 3)\n",
    "print(\"Train labels shape:\", train_labels.shape)  # Should be (num_samples,)\n",
    "print(train_labels)  # Should output integer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20f8bcc-6227-4b30-a6c3-8cc0e398ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/.local/lib/python3.10/site-packages/trimesh/grouping.py:99: RuntimeWarning: invalid value encountered in cast\n",
      "  stacked = np.column_stack(stacked).round().astype(np.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (12311, 1024, 3)\n",
      "Train labels shape: (12311,)\n",
      "Example labels: [1 0 1 1 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to load and preprocess a single .off file as a point cloud\n",
    "def load_off_file(off_filename, num_points=1024):\n",
    "    # Load the mesh using trimesh\n",
    "    mesh = trimesh.load_mesh(off_filename)\n",
    "    \n",
    "    # Sample points from the mesh\n",
    "    point_cloud = mesh.sample(num_points)\n",
    "    \n",
    "    # Normalize the point cloud (optional, but typically done)\n",
    "    point_cloud -= np.mean(point_cloud, axis=0)  # Centering\n",
    "    point_cloud /= np.max(np.linalg.norm(point_cloud, axis=1))  # Scaling\n",
    "    \n",
    "    return point_cloud\n",
    "\n",
    "# Function to load all .off files in a directory and their labels\n",
    "def load_off_dataset(directory, num_points=1024):\n",
    "    point_clouds = []\n",
    "    labels = []\n",
    "    \n",
    "    # Walk through the directory to find all .off files\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.off'):\n",
    "                # Full path to the file\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                # Extract label from the subdirectory (e.g., 'bottle' or 'chair')\n",
    "                label = os.path.basename(subdir)\n",
    "                \n",
    "                # Load the point cloud from the .off file\n",
    "                point_cloud = load_off_file(file_path, num_points=num_points)\n",
    "                \n",
    "                # Append the point cloud and label\n",
    "                point_clouds.append(point_cloud)\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    point_clouds = np.array(point_clouds)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return point_clouds, labels\n",
    "\n",
    "# Load the dataset\n",
    "dataset_dir = 'archive/ModelNet40/'\n",
    "train_data, train_labels = load_off_dataset(dataset_dir)\n",
    "\n",
    "# Use LabelEncoder to convert string labels to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)  # Converts string labels to integers\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(len(train_labels))\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_labels = train_labels[indices]\n",
    "\n",
    "# Check the shapes of the data\n",
    "print(\"Train data shape:\", train_data.shape)  # Should be (num_samples, 1024, 3)\n",
    "print(\"Train labels shape:\", train_labels.shape)  # Should be (num_samples,)\n",
    "print(\"Example labels:\", train_labels[:10])  # Should print the first 10 numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68826e35-2141-44fd-ba54-279f4c7400a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example labels: [22 37 35 ... 14 23 12]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example labels:\", train_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0740690c-9087-4a03-a088-6449af1d2aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 3s 42ms/step\n",
      "['train' 'train' 'train' 'train' 'train' 'train' 'train' 'train' 'train'\n",
      " 'train' 'train' 'train' 'train' 'train' 'train' 'train' 'train' 'train'\n",
      " 'train' 'train']\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into \"train\" and \"test\" sets for evaluation\n",
    "split_ratio = 0.8  # Use 80% for training and 20% for testing\n",
    "split_index = int(len(train_data) * split_ratio)\n",
    "\n",
    "# Train data\n",
    "train_data_split = train_data[:split_index]\n",
    "train_labels_split = train_labels[:split_index]\n",
    "\n",
    "# Test data\n",
    "test_data = train_data[split_index:]\n",
    "test_labels = train_labels[split_index:]\n",
    "\n",
    "# Predict class indices using the model on the test data\n",
    "predicted_labels = model.predict(test_data)\n",
    "predicted_class_indices = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "# Convert numeric labels back to string labels\n",
    "predicted_class_names = label_encoder.inverse_transform(predicted_class_indices)\n",
    "print(predicted_class_names[:20])  # Will print the original labels like 'bottle', 'chair'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2646c20-1dc4-4b72-a397-3666d4c7778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (2468, 1024, 3)\n",
      "Train labels shape: (2468,)\n",
      "Example labels: [22 37 35 34  6 22 37 26 25 14]\n"
     ]
    }
   ],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess a single .off file as a point cloud\n",
    "def load_off_file(off_filename, num_points=1024):\n",
    "    # Load the mesh using trimesh\n",
    "    mesh = trimesh.load_mesh(off_filename)\n",
    "    \n",
    "    # Sample points from the mesh\n",
    "    point_cloud = mesh.sample(num_points)\n",
    "    \n",
    "    # Normalize the point cloud (optional, but typically done)\n",
    "    point_cloud -= np.mean(point_cloud, axis=0)  # Centering\n",
    "    point_cloud /= np.max(np.linalg.norm(point_cloud, axis=1))  # Scaling\n",
    "    \n",
    "    return point_cloud\n",
    "\n",
    "# Function to load all .off files in a directory and their labels\n",
    "def load_off_dataset(directory, num_points=1024):\n",
    "    point_clouds = []\n",
    "    labels = []\n",
    "    \n",
    "    # Walk through the directory to find all .off files\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        # Skip the 'train' folder and process only class subdirectories\n",
    "        if os.path.basename(subdir) == 'train':\n",
    "            continue\n",
    "        \n",
    "        for file in files:\n",
    "            if file.endswith('.off'):\n",
    "                # Full path to the file\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                # Extract the class label from the parent directory\n",
    "                label = os.path.basename(os.path.dirname(subdir))\n",
    "                \n",
    "                # Load the point cloud from the .off file\n",
    "                point_cloud = load_off_file(file_path, num_points=num_points)\n",
    "                \n",
    "                # Append the point cloud and label\n",
    "                point_clouds.append(point_cloud)\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    point_clouds = np.array(point_clouds)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return point_clouds, labels\n",
    "\n",
    "# Load the dataset\n",
    "dataset_dir = 'archive/ModelNet40/'\n",
    "train_data, train_labels = load_off_dataset(dataset_dir)\n",
    "\n",
    "# Use LabelEncoder to convert string labels to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)  # Converts string labels to integers\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(len(train_labels))\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_labels = train_labels[indices]\n",
    "\n",
    "# Check the shapes of the data\n",
    "print(\"Train data shape:\", train_data.shape)  # Should be (num_samples, 1024, 3)\n",
    "print(\"Train labels shape:\", train_labels.shape)  # Should be (num_samples,)\n",
    "print(\"Example labels:\", train_labels[:10])  # Should print the first 10 numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e467e20-38bf-4142-b9c4-0c346eb453dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 4.2652 - accuracy: 0.0873 - val_loss: 2.7393 - val_accuracy: 0.2470\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - 14s 198ms/step - loss: 2.4374 - accuracy: 0.3003 - val_loss: 2.0571 - val_accuracy: 0.4130\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 1.9761 - accuracy: 0.4183 - val_loss: 1.6988 - val_accuracy: 0.4413\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 1.7061 - accuracy: 0.5061 - val_loss: 1.4549 - val_accuracy: 0.6073\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 1.4962 - accuracy: 0.5628 - val_loss: 1.2700 - val_accuracy: 0.6437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7af9097dc9d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs=5, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a051e6ac-04cc-4535-bb3c-d0e97a0f5712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 3s 42ms/step\n",
      "['bed' 'sofa' 'chair' ... 'bookshelf' 'dresser' 'bookshelf']\n"
     ]
    }
   ],
   "source": [
    "# Predict class indices using the model\n",
    "predicted_labels = model.predict(test_data)\n",
    "predicted_class_indices = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "# Convert numeric labels back to string labels\n",
    "predicted_class_names = label_encoder.inverse_transform(predicted_class_indices)\n",
    "print(predicted_class_names)  # Will print the original labels like 'bottle', 'chair'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "283dc6b0-2c4a-468c-b320-4f37654791a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "(1024, 3)\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Load a random point cloud from a .ply file\n",
    "random_cloud = o3d.io.read_point_cloud(\"filtered.pcd\")\n",
    "\n",
    "# Convert the point cloud to a numpy array\n",
    "random_points = np.asarray(random_cloud.points)\n",
    "\n",
    "# Check the number of points and downsample if necessary\n",
    "if random_points.shape[0] > 1024:\n",
    "    random_points = random_points[:1024, :]  # Select the first 1024 points\n",
    "\n",
    "# Normalize the point cloud (optional, depending on your preprocessing steps)\n",
    "random_points = random_points - np.mean(random_points, axis=0)  # Centering\n",
    "random_points = random_points / np.max(np.linalg.norm(random_points, axis=1))  # Scaling\n",
    "\n",
    "# Ensure the shape is (num_points, 3)\n",
    "print(random_points.shape)  # Should be (1024, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f34890-db37-4535-b7a1-10238f8f7469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted class for the random point cloud: [8]\n"
     ]
    }
   ],
   "source": [
    "# Add a batch dimension to the point cloud (model expects (batch_size, num_points, 3))\n",
    "random_points = np.expand_dims(random_points, axis=0)\n",
    "\n",
    "# Predict the label of each point (if it's segmentation) or the class (if it's classification)\n",
    "predicted_labels = model.predict(random_points)\n",
    "\n",
    "# If it's classification, the output will be a single label\n",
    "predicted_class = np.argmax(predicted_labels, axis=-1)\n",
    "print(f\"Predicted class for the random point cloud: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "defb8c5b-459e-4d70-9604-d97dc6d7497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predicted_labels: ()\n",
      "Error: predicted_labels is not an array.\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of predicted_labels to confirm it's a batch of labels\n",
    "print(f\"Shape of predicted_labels: {predicted_labels.shape}\")\n",
    "\n",
    "# Assuming predicted_labels is an array of integers (class indices)\n",
    "max_label = np.max(predicted_labels)  # Get the maximum label to normalize the colormap\n",
    "\n",
    "# Ensure predicted_labels is an array\n",
    "if isinstance(predicted_labels, np.ndarray):\n",
    "    # Get a colormap and apply it to the labels\n",
    "    cmap = plt.get_cmap(\"tab20\")  # Colormap with 20 discrete colors\n",
    "\n",
    "    # Apply the colormap to each predicted label\n",
    "    # For each predicted label, cmap returns an RGBA tuple, we convert that to a numpy array\n",
    "    colors = np.array([cmap(label / max_label) for label in predicted_labels])  # Normalize and get RGBA colors\n",
    "\n",
    "    # colors will have shape (num_points, 4) (RGBA), we only need the RGB part\n",
    "    colors = colors[:, :3]  # Extract the RGB channels (drop alpha)\n",
    "\n",
    "    # Assuming random_cloud is the Open3D point cloud object\n",
    "    random_cloud.colors = o3d.utility.Vector3dVector(colors)  # Apply the RGB colors\n",
    "\n",
    "    # Visualize the point cloud with the predicted labels\n",
    "    o3d.visualization.draw_geometries([random_cloud])\n",
    "else:\n",
    "    print(\"Error: predicted_labels is not an array.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d62087a7-31c1-44c5-ac9f-8b48fe35887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_data: (2463, 1024, 3)\n",
      "77/77 [==============================] - 3s 42ms/step\n",
      "Shape of predicted_class_indices: (2463,)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you are predicting on a point cloud\n",
    "# Ensure that test_data has the right shape (batch_size, num_points, 3)\n",
    "print(f\"Shape of test_data: {test_data.shape}\")  # Check the shape of the input data\n",
    "\n",
    "# Ensure test_data has a batch dimension\n",
    "if len(test_data.shape) == 2:  # If the shape is (num_points, 3)\n",
    "    test_data = np.expand_dims(test_data, axis=0)  # Add batch dimension\n",
    "\n",
    "# Now predict\n",
    "predicted_labels = model.predict(test_data)\n",
    "predicted_class_indices = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "# Now predicted_class_indices should be an array of labels\n",
    "print(f\"Shape of predicted_class_indices: {predicted_class_indices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb413c47-fdf5-448f-b067-cb531a06c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that test_data has the correct shape\n",
    "if len(test_data.shape) == 2:\n",
    "    test_data = np.expand_dims(test_data, axis=0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03b4171e-f9d4-4d2a-8c8d-e2b895852911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 3s 43ms/step\n",
      "Shape of predicted_class_indices: (2463,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m max_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(predicted_class_indices)\n\u001b[1;32m     11\u001b[0m cmap \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtab20\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m colors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([cmap(label \u001b[38;5;241m/\u001b[39m max_label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m predicted_class_indices[\u001b[38;5;241m0\u001b[39m]])  \u001b[38;5;66;03m# For the first point cloud in the batch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m colors \u001b[38;5;241m=\u001b[39m colors[:, :\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# Extract the RGB channels\u001b[39;00m\n\u001b[1;32m     16\u001b[0m random_cloud\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(colors)  \u001b[38;5;66;03m# Apply the RGB colors\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Predict labels using the corrected test_data shape\n",
    "predicted_labels = model.predict(test_data)\n",
    "predicted_class_indices = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "# Check the shape of the predicted_class_indices\n",
    "print(f\"Shape of predicted_class_indices: {predicted_class_indices.shape}\")\n",
    "\n",
    "# Now apply the colormap as before\n",
    "max_label = np.max(predicted_class_indices)\n",
    "\n",
    "cmap = plt.get_cmap(\"tab20\")\n",
    "colors = np.array([cmap(label / max_label) for label in predicted_class_indices[0]])  # For the first point cloud in the batch\n",
    "\n",
    "colors = colors[:, :3]  # Extract the RGB channels\n",
    "\n",
    "random_cloud.colors = o3d.utility.Vector3dVector(colors)  # Apply the RGB colors\n",
    "\n",
    "# Visualize the point cloud with the predicted labels\n",
    "o3d.visualization.draw_geometries([random_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1683f3bb-c751-4dd9-aade-780f49456386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 3s 42ms/step\n",
      "Shape of predicted_class_indices: (2463,)\n",
      "Error: predicted_class_indices has an unexpected shape. Please check your model output.\n"
     ]
    }
   ],
   "source": [
    "# Predict labels using the model\n",
    "predicted_labels = model.predict(test_data)\n",
    "predicted_class_indices = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "# Check the shape of the predicted labels to understand what the model is returning\n",
    "print(f\"Shape of predicted_class_indices: {predicted_class_indices.shape}\")\n",
    "\n",
    "# If the shape is correct, continue with colormap application\n",
    "if len(predicted_class_indices.shape) == 2:  # (batch_size, num_points)\n",
    "    # Now apply the colormap to the first point cloud in the batch\n",
    "    max_label = np.max(predicted_class_indices)\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    \n",
    "    # Apply the colormap to each label in the first point cloud\n",
    "    colors = np.array([cmap(label / max_label) for label in predicted_class_indices[0]])  # First point cloud\n",
    "    \n",
    "    # colors will have shape (num_points, 4), we only need the RGB part\n",
    "    colors = colors[:, :3]  # Extract RGB channels\n",
    "    \n",
    "    # Assuming random_cloud is the Open3D point cloud object\n",
    "    random_cloud.colors = o3d.utility.Vector3dVector(colors)  # Apply the RGB colors\n",
    "    \n",
    "    # Visualize the point cloud with the predicted labels\n",
    "    o3d.visualization.draw_geometries([random_cloud])\n",
    "else:\n",
    "    print(\"Error: predicted_class_indices has an unexpected shape. Please check your model output.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
