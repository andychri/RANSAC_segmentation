{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1bae2b3-03e5-4d72-a227-8df1a2025408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Path to the point cloud file (replace with the correct file path)\n",
    "file_path = \"archive(1)/bildstein_station1_xyz_intensity_rgb/bildstein_station1_xyz_intensity_rgb.txt\"\n",
    "\n",
    "# Load the point cloud (assuming it's a .txt file with XYZ + intensity + RGB)\n",
    "data = np.loadtxt(file_path)\n",
    "\n",
    "# Extract XYZ coordinates and RGB values\n",
    "points = data[:, 0:3]  # First three columns are XYZ coordinates\n",
    "intensity = data[:, 3]  # Intensity (optional, may skip if not needed)\n",
    "colors = data[:, 4:7]   # Next three columns are RGB values\n",
    "\n",
    "# Create Open3D point cloud\n",
    "#pcd = o3d.geometry.PointCloud()\n",
    "#pcd.points = o3d.utility.Vector3dVector(points)\n",
    "#pcd.colors = o3d.utility.Vector3dVector(colors / 255.0)  # Normalize colors to [0, 1]\n",
    "\n",
    "# Visualize the point cloud\n",
    "#o3d.visualization.draw_geometries([pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcbdccd-b187-48a3-9daf-71eb859297dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 6. 0. 0. 0. 0. 6. 6. 6.]\n",
      "[0. 0. 6. 0. 0. 0. 0. 6. 6. 6.]\n"
     ]
    }
   ],
   "source": [
    "labels_file_path = \"archive(1)/sem8_labels_training/sem8_labels_training/bildstein_station1_xyz_intensity_rgb.labels\"\n",
    "# Load the labels (assuming one label per point)\n",
    "labels = np.loadtxt(labels_file_path)\n",
    "\n",
    "# Print first few labels to check\n",
    "print(labels[:10])\n",
    "\n",
    "# You can visualize labels by assigning different colors to different classes (optional)\n",
    "# For now, this just prints them, but further visualization of segmentation can be added later\n",
    "\n",
    "# Load the labels (assuming one label per point)\n",
    "labels = np.loadtxt(labels_file_path)\n",
    "\n",
    "# Print first few labels to check\n",
    "print(labels[:10])\n",
    "\n",
    "# You can visualize labels by assigning different colors to different classes (optional)\n",
    "# For now, this just prints them, but further visualization of segmentation can be added later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ec26df-ccdc-48db-b4ad-7537dd6c836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, point_cloud_file, label_file):\n",
    "        self.points = np.loadtxt(point_cloud_file)[:, :3]  # XYZ coordinates\n",
    "        self.labels = np.loadtxt(label_file)  # Corresponding labels for each point\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point = self.points[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(point, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Example usage with one of your point clouds\n",
    "point_cloud_file = \"archive(1)/bildstein_station1_xyz_intensity_rgb/bildstein_station1_xyz_intensity_rgb.txt\"\n",
    "label_file = \"archive(1)/sem8_labels_training/sem8_labels_training/bildstein_station1_xyz_intensity_rgb.labels\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = PointCloudDataset(point_cloud_file, label_file)\n",
    "\n",
    "# Create a DataLoader to batch and shuffle the data\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cf7167-f300-4d9e-9cdc-ad120294e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 17:19:49.366547: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-02 17:19:49.606475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 17:19:49.606525: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 17:19:49.641228: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 17:19:49.715311: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 17:19:50.545139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points shape: (29697591, 3), Labels shape: (29697591,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load point cloud (assume you have already loaded it into 'points' and 'labels' arrays)\n",
    "# Example data loading from earlier\n",
    "points = np.loadtxt(\"archive(1)/bildstein_station1_xyz_intensity_rgb/bildstein_station1_xyz_intensity_rgb.txt\")[:, 0:3]\n",
    "labels = np.loadtxt(\"archive(1)/sem8_labels_training/sem8_labels_training/bildstein_station1_xyz_intensity_rgb.labels\")\n",
    "\n",
    "# Optionally downsample or normalize points\n",
    "points -= np.mean(points, axis=0)  # Center the point cloud\n",
    "points /= np.max(np.linalg.norm(points, axis=1))  # Scale to unit sphere\n",
    "\n",
    "# Reshape and prepare the dataset for TensorFlow (ensure the shape is (num_points, 3) and labels are (num_points,))\n",
    "print(f\"Points shape: {points.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "# Shuffle and batch the data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((points, labels))\n",
    "dataset = dataset.shuffle(buffer_size=len(points)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc5feb2-8486-4cc1-89c5-742931acff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of points: 29697591\n",
      "Number of points after downsampling: 893679\n",
      "Downsampled number of points: 893679\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# Ensure 'points' contains the original point cloud data (Reset each time)\n",
    "original_points = np.copy(points)  # Create a copy of the original points to ensure it's not modified\n",
    "\n",
    "# Load the original point cloud from the copy\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(original_points)\n",
    "\n",
    "# Print the number of points in the original point cloud\n",
    "print(f\"Original number of points: {len(original_points)}\")\n",
    "\n",
    "# Apply voxel downsampling\n",
    "voxel_size = 0.0006  # Adjust voxel size as needed\n",
    "downsampled_pcd = pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "downsampled_points = np.asarray(downsampled_pcd.points)\n",
    "\n",
    "# Print the number of points after downsampling\n",
    "print(f\"Number of points after downsampling: {len(downsampled_points)}\")\n",
    "\n",
    "# Use KDTree to find the nearest original points corresponding to the downsampled points\n",
    "kdtree = KDTree(original_points)  # Use the original points, not the downsampled\n",
    "distances, indices = kdtree.query(downsampled_points)\n",
    "\n",
    "# The \"indices\" array contains the indices of the original points that correspond to the downsampled points\n",
    "downsampled_labels = labels[indices]\n",
    "\n",
    "print(f\"Downsampled number of points: {len(downsampled_points)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e52bcc2f-0f9d-4536-9cdd-62cb8be86e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_pointnet_segmentation_model(num_classes, input_shape=(None, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Add more layers and dropout to improve generalization\n",
    "    x = layers.Conv1D(128, kernel_size=1, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Conv1D(256, kernel_size=1, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Conv1D(512, kernel_size=1, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    num_points = tf.shape(inputs)[1]\n",
    "    global_features_repeated = tf.tile(tf.expand_dims(global_features, axis=1), [1, num_points, 1])\n",
    "\n",
    "    x = layers.Concatenate()([inputs, global_features_repeated])\n",
    "\n",
    "    x = layers.Conv1D(256, kernel_size=1, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Conv1D(128, kernel_size=1, activation='relu')(x)\n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b285948-84cd-4d9e-90f3-b078ac4c1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Add a batch dimension to your downsampled point cloud and labels\n",
    "downsampled_points = np.expand_dims(downsampled_points, axis=0)  # Shape (1, num_points, 3)\n",
    "downsampled_labels = np.expand_dims(downsampled_labels, axis=0)  # Shape (1, num_points)\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((downsampled_points, downsampled_labels)).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18b32067-4534-4be1-9f6f-d68cde0c6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, None, 3)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)          (None, None, 64)             256       ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)          (None, None, 128)            8320      ['conv1d_36[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)          (None, None, 256)            33024     ['conv1d_37[0][0]']           \n",
      "                                                                                                  \n",
      " global_max_pooling1d_6 (Gl  (None, 256)                  0         ['conv1d_38[0][0]']           \n",
      " obalMaxPooling1D)                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_6 (TFOp  (3,)                         0         ['input_7[0][0]']             \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.expand_dims_6 (TFOpLamb  (None, 1, 256)               0         ['global_max_pooling1d_6[0][0]\n",
      " da)                                                                ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6  ()                           0         ['tf.compat.v1.shape_6[0][0]']\n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.tile_6 (TFOpLambda)      (None, None, 256)            0         ['tf.expand_dims_6[0][0]',    \n",
      "                                                                     'tf.__operators__.getitem_6[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, None, 259)            0         ['input_7[0][0]',             \n",
      " )                                                                   'tf.tile_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)          (None, None, 128)            33280     ['concatenate_6[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_40 (Conv1D)          (None, None, 64)             8256      ['conv1d_39[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_41 (Conv1D)          (None, None, 9)              585       ['conv1d_40[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 83721 (327.04 KB)\n",
      "Trainable params: 83721 (327.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the PointNet++ model\n",
    "def build_pointnet_segmentation_model(num_classes, input_shape=(None, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # PointNet++ architecture (simplified for segmentation)\n",
    "    x = layers.Conv1D(64, kernel_size=1, activation='relu')(inputs)\n",
    "    x = layers.Conv1D(128, kernel_size=1, activation='relu')(x)\n",
    "    x = layers.Conv1D(256, kernel_size=1, activation='relu')(x)\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Use tf.tile to repeat the global features dynamically for each point\n",
    "    num_points = tf.shape(inputs)[1]\n",
    "    global_features_repeated = tf.tile(tf.expand_dims(global_features, axis=1), [1, num_points, 1])\n",
    "\n",
    "    # Concatenate global features with point features\n",
    "    x = layers.Concatenate()([inputs, global_features_repeated])\n",
    "\n",
    "    # Final point-wise segmentation layers\n",
    "    x = layers.Conv1D(128, kernel_size=1, activation='relu')(x)\n",
    "    x = layers.Conv1D(64, kernel_size=1, activation='relu')(x)\n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(np.unique(downsampled_labels))  # Adjust for your dataset\n",
    "model = build_pointnet_segmentation_model(num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary to verify\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "530f2a65-70be-4fc7-b409-aa0f00eace34",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)  # Reduce learning rate\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "042b1c61-aaa2-4525-8366-6e7bbc31bbbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "too many positional arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m class_weight\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Compute class weights to handle class imbalance\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownsampled_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsampled_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m class_weights_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(class_weights))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Add class weights during training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_sig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3186\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3112\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3110\u001b[0m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m         \u001b[38;5;66;03m# argument\u001b[39;00m\n\u001b[0;32m-> 3112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3113\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m _VAR_POSITIONAL:\n\u001b[1;32m   3116\u001b[0m         \u001b[38;5;66;03m# We have an '*args'-like argument, let's fill it with\u001b[39;00m\n\u001b[1;32m   3117\u001b[0m         \u001b[38;5;66;03m# all positional arguments we have left and move on to\u001b[39;00m\n\u001b[1;32m   3118\u001b[0m         \u001b[38;5;66;03m# the next phase\u001b[39;00m\n\u001b[1;32m   3119\u001b[0m         values \u001b[38;5;241m=\u001b[39m [arg_val]\n",
      "\u001b[0;31mTypeError\u001b[0m: too many positional arguments"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(downsampled_labels), downsampled_labels.flatten())\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Add class weights during training\n",
    "model.fit(dataset, epochs=20, class_weight=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8a0b6f0-6bd6-419a-867c-1dd38ba91e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2339 - accuracy: 0.7213\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.3638 - accuracy: 0.0928\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5470 - accuracy: 0.7213\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.3325 - accuracy: 0.7213\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1686 - accuracy: 0.7213\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5548 - accuracy: 0.7160\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4773 - accuracy: 0.7213\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.3183 - accuracy: 0.7213\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0887 - accuracy: 0.7213\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1768 - accuracy: 0.7213\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0314 - accuracy: 0.7213\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0193 - accuracy: 0.7213\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0193 - accuracy: 0.7213\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# Adjust based on your needs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m  \u001b[38;5;66;03m# Adjust based on your hardware capabilities\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the PointNet++ segmentation model\n",
    "epochs = 20  # Adjust based on your needs\n",
    "batch_size = 32  # Adjust based on your hardware capabilities\n",
    "model.fit(dataset, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7601af77-7f0b-4f55-add4-666e223f87c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict segmentation labels for the point cloud\n",
    "predicted_labels = model.predict(downsampled_points)  # Predict for the downsampled points\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "predicted_labels = np.argmax(predicted_labels, axis=-1).flatten()  # Shape: (num_points,)\n",
    "\n",
    "# Print the first few predicted labels to verify\n",
    "print(predicted_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b85daec-a74a-48a7-8de2-528da2fed11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Create an Open3D PointCloud object\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(downsampled_points[0])  # Use the downsampled points\n",
    "\n",
    "# Assign random colors to each class label\n",
    "num_classes = len(np.unique(predicted_labels))\n",
    "colors = np.random.rand(num_classes, 3)  # Generate random colors for each class\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors[predicted_labels])\n",
    "\n",
    "# Visualize the segmented point cloud\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
